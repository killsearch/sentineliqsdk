---
description: Pipeline development patterns and integration requirements
---

# Pipeline Development Patterns

This rule defines the patterns and requirements for implementing Pipeline modules in the SentinelIQ SDK.

## File Organization

### Pipelines
- **Code**: `src/sentineliqsdk/pipelines/<name>.py`
- **Example**: `examples/pipelines/<name>_example.py`
- **Tests**: `tests/pipelines/test_<name>.py`
- **Class naming**: `<Name>Pipeline` extending `sentineliqsdk.pipelines.Pipeline` or `PipelineOrchestrator`

## Required Imports and Structure

### Pipeline Base Pattern
```python
from __future__ import annotations

from sentineliqsdk.messaging import Message
from sentineliqsdk.models import ModuleMetadata, WorkerInput
from sentineliqsdk.pipelines.base import Pipeline

class MyPipeline(Pipeline):
    METADATA = ModuleMetadata(
        name="My Pipeline",
        description="Description of what this pipeline does",
        author=("SentinelIQ Team <team@sentineliq.com.br>",),
        pattern="pipeline",
        doc_pattern="Pipeline documentation pattern",
        doc="https://killsearch.github.io/sentineliqsdk/modulos/pipelines/my_pipeline/",
        version_stage="TESTING",
    )

    def process_message(self, message: Message) -> dict[str, Any]:
        """Process a message through the pipeline."""
        # Implementation here
        pass

    def run(self) -> None:
        """Override to implement pipeline logic."""
        # Implementation here
        pass
```

### Pipeline Orchestrator Pattern
```python
from __future__ import annotations

from sentineliqsdk.analyzers import Analyzer
from sentineliqsdk.messaging import Message
from sentineliqsdk.models import ModuleMetadata, WorkerInput
from sentineliqsdk.pipelines.orchestrator import PipelineOrchestrator
from sentineliqsdk.responders import Responder

class MyOrchestratorPipeline(PipelineOrchestrator):
    METADATA = ModuleMetadata(
        name="My Orchestrator Pipeline",
        description="Orchestrates analyzers and responders",
        author=("SentinelIQ Team <team@sentineliq.com.br>",),
        pattern="orchestrator",
        doc_pattern="Orchestrator pipeline documentation",
        doc="https://killsearch.github.io/sentineliqsdk/modulos/pipelines/my_orchestrator/",
        version_stage="TESTING",
    )

    def __init__(self, input_data: WorkerInput, secret_phrases=None):
        super().__init__(input_data, secret_phrases)
        self._setup_analyzers()
        self._setup_responders()
        self._setup_routing_rules()

    def _setup_analyzers(self) -> None:
        """Setup analyzers for different data types."""
        self.register_analyzer("ip", ShodanAnalyzer)
        self.register_analyzer("domain", ShodanAnalyzer)

    def _setup_responders(self) -> None:
        """Setup responders for different actions."""
        self.register_responder("block", BlockIPResponder)
        self.register_responder("alert", NotifyResponder)

    def _setup_routing_rules(self) -> None:
        """Setup routing rules for message processing."""
        def routing_rule(message: Message) -> str:
            if message.metadata.priority == "critical":
                return "immediate_response"
            return "standard_analysis"
        
        self.add_routing_rule(routing_rule)
```

## Configuration Requirements

### Pipeline Configuration
```python
# Pipeline input configuration
input_data = WorkerInput(
    data_type="other",
    data="pipeline_name",
    config=WorkerConfig(
        params={
            "auto_respond": True,
            "response_threshold": "suspicious",
            "max_retries": 3,
        },
        secrets={
            "kafka.bootstrap_servers": "localhost:9092",
            "kafka.security_protocol": "PLAINTEXT",
            "analyzer.api_key": "api_key_value",
        }
    ),
)

pipeline = MyPipeline(input_data)
```

### Message Processing
```python
def process_message(self, message: Message) -> dict[str, Any]:
    """Process a message through the pipeline."""
    try:
        # 1. Validate message
        if not message.data:
            self._record_failure()
            return {
                "error": "Message data cannot be empty",
                "message_id": message.metadata.message_id,
            }

        # 2. Process through pipeline logic
        result = self._process_pipeline_logic(message)

        # 3. Record success
        self._record_success()

        return {
            "message_id": message.metadata.message_id,
            "data_type": message.data_type,
            "data": message.data,
            "pipeline_result": result,
            "pipeline_status": "completed",
            "processing_time": time.time(),
            "metadata": self.METADATA.to_dict(),
        }

    except Exception as e:
        self._record_failure()
        return {
            "message_id": message.metadata.message_id,
            "error_message": str(e),
            "pipeline_status": "failed",
            "metadata": self.METADATA.to_dict(),
        }
```

## Integration Patterns

### Producer Integration
```python
from sentineliqsdk.producers import KafkaProducer
from sentineliqsdk.messaging import QueueConfig, MessageConfig

# Setup producer
producer_input = WorkerInput(
    data_type="other",
    data="event_producer",
    config=WorkerConfig(secrets={"kafka.bootstrap_servers": "localhost:9092"})
)

producer = KafkaProducer(producer_input)
producer.configure_queue(QueueConfig(queue_name="events"))
producer.configure_messaging(MessageConfig(delivery_mode="persistent"))

# Publish event
event_message = producer.build_message(
    message_type="event",
    data_type="ip",
    data="192.168.1.100",
    priority="high",
    tags={"source": "firewall"}
)

producer_report = producer.publish(event_message)
```

### Consumer Integration
```python
from sentineliqsdk.consumers import PipelineConsumer
from sentineliqsdk.pipelines import SecurityPipeline

# Setup pipeline
pipeline = SecurityPipeline(pipeline_input)

# Setup consumer with pipeline
consumer_input = WorkerInput(
    data_type="other",
    data="pipeline_consumer",
    config=WorkerConfig(secrets={"kafka.bootstrap_servers": "localhost:9092"})
)

consumer = PipelineConsumer(consumer_input, pipeline=pipeline)
consumer.configure_queue(QueueConfig(queue_name="events"))
consumer.configure_messaging(MessageConfig(auto_ack=False))

# Process messages
report = consumer.start_consuming()
```

## Example Requirements

### Pipeline Example Structure
```python
from __future__ import annotations

import argparse
import json
import sys
import time

from sentineliqsdk import WorkerConfig, WorkerInput
from sentineliqsdk.messaging import Message, MessageConfig, MessageMetadata, QueueConfig
from sentineliqsdk.pipelines import MyPipeline

def main() -> None:
    parser = argparse.ArgumentParser(description="My Pipeline Example")
    parser.add_argument("--data", default="test_data", help="Data to process")
    parser.add_argument("--topic", default="my-topic", help="Topic name")
    parser.add_argument("--execute", action="store_true", help="Execute real processing")
    parser.add_argument("--include-dangerous", action="store_true", help="Include dangerous operations")
    
    args = parser.parse_args()
    
    # Create pipeline
    input_data = WorkerInput(
        data_type="other",
        data="pipeline_name",
        config=WorkerConfig(
            params={
                "auto_respond": args.include_dangerous,
                "response_threshold": "suspicious",
            },
            secrets={
                "kafka.bootstrap_servers": "localhost:9092",
                "kafka.security_protocol": "PLAINTEXT",
                "analyzer.api_key": "your_api_key",
            }
        ),
    )
    
    pipeline = MyPipeline(input_data)
    
    if args.execute:
        try:
            # Create test message
            message = Message(
                message_type="event",
                data_type="ip",
                data=args.data,
                metadata=MessageMetadata(
                    message_id=f"test_{int(time.time())}",
                    priority="normal",
                    tags={"source": "example"}
                ),
                payload={"secrets": {"analyzer.api_key": "your_api_key"}}
            )
            
            # Process through pipeline
            result = pipeline.process_message(message)
            
            print(json.dumps(result, ensure_ascii=False, indent=2))
            
        except Exception as e:
            error_result = {
                "success": False,
                "error": str(e),
                "message": "Failed to process message"
            }
            print(json.dumps(error_result, ensure_ascii=False, indent=2))
            sys.exit(1)
    else:
        # Dry run
        dry_run_result = {
            "mode": "dry_run",
            "message": "Would process message through pipeline:",
            "data": args.data,
            "topic": args.topic,
            "pipeline": "MyPipeline",
            "note": "Use --execute to actually process the message"
        }
        print(json.dumps(dry_run_result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()
```

## Error Handling Patterns

### Pipeline Error Handling
```python
def process_message(self, message: Message) -> dict[str, Any]:
    try:
        # Validate inputs
        if not message.data:
            raise ValueError("Message data cannot be empty")
        
        # Process pipeline logic
        result = self._process_logic(message)
        
        # Record success
        self._record_success()
        
        return {
            "message_id": message.metadata.message_id,
            "pipeline_result": result,
            "pipeline_status": "completed",
            "metadata": self.METADATA.to_dict(),
        }
        
    except ValueError as e:
        # Validation error
        self._record_failure()
        return {
            "message_id": message.metadata.message_id,
            "error_message": f"Validation error: {e}",
            "pipeline_status": "failed",
            "metadata": self.METADATA.to_dict(),
        }
        
    except Exception as e:
        # Unexpected error
        self._record_failure()
        return {
            "message_id": message.metadata.message_id,
            "error_message": f"Pipeline error: {e}",
            "pipeline_status": "failed",
            "metadata": self.METADATA.to_dict(),
        }
```

## Statistics and Monitoring

### Pipeline Statistics
```python
def summary(self, raw: Any) -> dict:
    """Return pipeline-specific statistics."""
    return {
        "messages_processed": self._processing_stats["messages_processed"],
        "messages_failed": self._processing_stats["messages_failed"],
        "processing_time": time.time() - self._processing_stats["start_time"],
        "registered_analyzers": len(self.analyzers),
        "registered_responders": len(self.responders),
        "routing_rules": len(self.routing_rules),
    }
```

### Performance Monitoring
```python
# Monitor pipeline performance
summary = pipeline.summary({})
print(f"Throughput: {summary['messages_processed'] / summary['processing_time']:.2f} msg/s")
print(f"Success rate: {summary['messages_processed'] / (summary['messages_processed'] + summary['messages_failed']) * 100:.1f}%")
```

## Testing Patterns

### Pipeline Testing
```python
def test_pipeline_process_message():
    """Test message processing through pipeline."""
    input_data = WorkerInput(
        data_type="other",
        data="test_pipeline",
        config=WorkerConfig(secrets={"analyzer.api_key": "test_key"})
    )
    
    pipeline = MyPipeline(input_data)
    
    message = Message(
        message_type="event",
        data_type="ip",
        data="192.168.1.100",
        metadata=MessageMetadata(message_id="test_msg"),
    )
    
    result = pipeline.process_message(message)
    
    assert result["pipeline_status"] == "completed"
    assert result["message_id"] == "test_msg"
    assert "pipeline_result" in result
```

### Integration Testing
```python
def test_pipeline_consumer_integration():
    """Test pipeline integration with consumer."""
    # Setup pipeline
    pipeline_input = WorkerInput(
        data_type="other",
        data="test_pipeline",
        config=WorkerConfig(secrets={"analyzer.api_key": "test_key"})
    )
    pipeline = MyPipeline(pipeline_input)
    
    # Setup consumer
    consumer_input = WorkerInput(
        data_type="other",
        data="test_consumer",
        config=WorkerConfig(secrets={"kafka.bootstrap_servers": "localhost:9092"})
    )
    consumer = PipelineConsumer(consumer_input, pipeline=pipeline)
    
    # Test message processing
    message = Message(
        message_type="event",
        data_type="ip",
        data="192.168.1.100",
        metadata=MessageMetadata(message_id="test_msg"),
    )
    
    report = consumer.consume(message)
    
    assert report.success is True
    assert report.messages_processed == 1
```

## Documentation Requirements

### Required Documentation Files
- `docs/modulos/pipelines/<name>.md` - Pipeline documentation

### Documentation Structure
Each documentation file must include:
- **Characteristics**: Key features and capabilities
- **Basic Usage**: Simple example with `WorkerInput` and `WorkerConfig.secrets`
- **Configuration**: All available parameters and secrets
- **Integration**: Examples with Producer/Consumer integration
- **Error Handling**: How errors are handled
- **Monitoring**: Statistics and performance metrics
- **Dependencies**: Required packages

### Navigation Updates
Update `mkdocs.yml` navigation under the "Modules" section:
```yaml
nav:
  - Modules:
    - Pipelines:
      - Security Pipeline: modulos/pipelines/security_pipeline.md
      - Custom Pipeline: modulos/pipelines/custom_pipeline.md
```

## Checklist for New Pipelines

### Code Requirements
- [ ] Naming and imports compliant
- [ ] `METADATA` attribute declared and included in full_report
- [ ] `process_message()` implemented
- [ ] Calls `self.report(...)` with appropriate report type
- [ ] Error handling implemented
- [ ] Statistics tracking implemented

### Testing
- [ ] Tests added under `tests/pipelines/test_<name>.py`
- [ ] `poe lint` passes
- [ ] `poe test` passes

### Examples
- [ ] Example under `examples/pipelines/` runnable
- [ ] Prints compact result to STDOUT
- [ ] Supports `--execute` for real processing
- [ ] Supports `--include-dangerous` for impactful operations
- [ ] Uses `WorkerConfig.secrets` for credentials (never `os.environ`)

### Documentation
- [ ] Docs updated (Guides/Tutorials/Examples/Reference)
- [ ] Links added to relevant pages
- [ ] `mkdocs.yml` updated if needed
- [ ] `poe docs` passes locally
- [ ] Programmatic docs page added: `docs/modulos/pipelines/<name>.md`